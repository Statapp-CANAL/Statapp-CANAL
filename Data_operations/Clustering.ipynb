{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PCA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPCA\u001b[39;00m \n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PCA'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PCA\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_dataframe(filenames,st = ','): \n",
    "    \"\"\"\n",
    "    Initialize the DataFrame from a filenames \n",
    "    Parameters: \n",
    "    -----------\n",
    "    filenames: str, the name of the file\n",
    "    st: str, helps to delimites the columns of the datas because \"Correspondances_promo\n",
    "    are delimited with ';' while the other are delimited with ','.\n",
    "    \"\"\"\n",
    "    datas = pd.read_csv(filenames,delimiter=st)\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv_file(df,filename):\n",
    "    \"\"\"\n",
    "    Save a dataframe on a .csv file at filename\n",
    "    ----------\n",
    "    df: dataframe, the DataFrame that you want to save\n",
    "    filename: str, the place where you want to save your data frame\n",
    "    \"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = file_to_dataframe(data_path + \"exemple.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Sélection du nombre de clusters (ici, on suppose K=8 par exemple)\n",
    "kmeans = KMeans(n_clusters=8, random_state=42)\n",
    "kmeans.fit(df_scaled)\n",
    "\n",
    "# Ajout des étiquettes de cluster au DataFrame original\n",
    "df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Évaluation du modèle avec le score de silhouette\n",
    "score = silhouette_score(df_scaled, kmeans.labels_)\n",
    "print(f'Score de silhouette: {score}')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score de silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "indices = np.random.choice(range(len(df_scaled)), size=int(len(df_scaled) * 0.1), replace=False)\n",
    "sample = df_scaled.iloc[indices]\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2, 11):  # Testez des valeurs de k de 2 à 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(sample)\n",
    "    score = silhouette_score(sample, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "    print('Score calculé pour k =', k)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.xlabel('Nombre de clusters')\n",
    "plt.ylabel('Score de silhouette KMeans')\n",
    "plt.title('Score de silhouette pour différents nombres de clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methode du Coude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):  # Test de 1 à 10 clusters\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(df_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Affichage de la méthode du coude\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
    "plt.title('Méthode du Coude pour le choix optimal de K')\n",
    "plt.xlabel('Nombre de clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse par composantes principales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)  # Réduction à 2 dimensions pour la visualisation\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_pca[:, 0], df_pca[:, 1], alpha=0.7)\n",
    "plt.xlabel('Première composante principale')\n",
    "plt.ylabel('Deuxième composante principale')\n",
    "plt.title('Visualisation ACP des données')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data_set(filename, columns, change_inf=np.nan, change_nan=15):\n",
    "    \"\"\"\n",
    "    Load a dataset, and return scaled data.\n",
    "\n",
    "    Args:\n",
    "    - filename: Name of the file containing the data.\n",
    "    - columns: List of column names to include in clustering.\n",
    "    - change_inf: Value to use for replacing infinity.\n",
    "    - change_nan: Value to use for replacing NaN.\n",
    "\n",
    "    Returns:\n",
    "    - data: DataFrame containing clustered data.\n",
    "    \"\"\"\n",
    "    # Load the dataset from file\n",
    "    df = file_to_dataframe(filename)\n",
    "    \n",
    "    # Select the specified columns\n",
    "    data = df[columns]\n",
    "    \n",
    "    # Replace infinite values with specified value\n",
    "    data.replace([np.inf, -np.inf], change_inf, inplace=True)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    datas = scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(datas, columns=data.columns)\n",
    "\n",
    "    # Select a random 10% sample of the data\n",
    "    indices = np.random.choice(range(len(data)), size=int(len(data) * 0.1), replace=False)\n",
    "    data = data.iloc[indices]\n",
    "\n",
    "    # Replace NaN values with specified value\n",
    "    data.replace(np.nan, change_nan, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction auxiliaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_abo_conditions(df: pd.DataFrame,columns ,cond):\n",
    "    \"\"\"\n",
    "    This function count the number of elements of the condition and group by\n",
    "    the column \n",
    "    \"\"\"\n",
    "    series = df.groupby(columns)[cond].count() # Group by specified columns and count occurrences of the condition\n",
    "    df_filtered_groups = series.reset_index(name = 'POURCENTAGE_' + cond) #Series into DataFrames\n",
    "\n",
    "    df_filtered_groups = df_filtered_groups.sort_values(by = 'POURCENTAGE_' + cond, ascending=False) # Sort the DataFrame by the count column in descending order\n",
    "\n",
    "    n = df_filtered_groups['POURCENTAGE_' + cond].sum()\n",
    "    df_filtered_groups['POURCENTAGE_' + cond] = df_filtered_groups['POURCENTAGE_' + cond] / n * 100\n",
    "\n",
    "    df_filtered_groups = df_filtered_groups[df_filtered_groups['POURCENTAGE_' + cond] >= 0.01]\n",
    "\n",
    "    return df_filtered_groups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renvoie les informations sur les centres des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_frame_cluster(data, columns, centers_inv, clusters, data_id_abo):\n",
    "    \"\"\"\n",
    "    Create a DataFrame containing clusters, cluster centers, and subscriber IDs.\n",
    "\n",
    "    Args:\n",
    "    - data: DataFrame containing the data.\n",
    "    - columns: List of column names.\n",
    "    - centers_inv: Inverse cluster centers (denormalized).\n",
    "    - clusters: Cluster number assigned to each sample.\n",
    "    - data_id_abo: DataFrame containing subscriber IDs.\n",
    "\n",
    "    Returns:\n",
    "    - df_clusters: DataFrame containing clusters, centers, and percentage IDs.\n",
    "    \"\"\"\n",
    "    # Add cluster labels and subscriber IDs to the data\n",
    "    data['KMEANS'] = clusters\n",
    "    data['ID_ABONNE'] = data_id_abo['ID_ABONNE']\n",
    "\n",
    "    # Generate DataFrame with cluster information\n",
    "    df_clusters = percent_abo_conditions(data, 'KMEANS', 'ID_ABONNE')\n",
    "    df_clusters = df_clusters.sort_values(by='KMEANS')\n",
    "\n",
    "    # Round and assign cluster centers to the DataFrame\n",
    "    centers = np.round(centers_inv, decimals=2)\n",
    "    for j in range(len(columns)):\n",
    "        df_clusters[columns[j]] = [centers[i][j] for i in range(len(centers))]\n",
    "\n",
    "    return df_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renvoie des fichiers avec les informations sur les clusters + visualisation des scores de silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_silhouette_datas_all(filename, columns, range_n_clusters, data_path_results, change_inf=np.nan, change_nan=15):\n",
    "    \"\"\"\n",
    "    Visualize silhouette scores for different numbers of clusters.\n",
    "\n",
    "    Args:\n",
    "    - filename: Name of the file containing the data.\n",
    "    - columns: List of column names to include in clustering.\n",
    "    - range_n_clusters: List of numbers of clusters to test.\n",
    "    - data_path_results: Path to save the results.\n",
    "    - change_inf: Value to use for replacing infinity.\n",
    "    - change_nan: Value to use for replacing NaN.\n",
    "\n",
    "    Returns:\n",
    "    - silhouette_scores: List of silhouette scores.\n",
    "    \"\"\"\n",
    "    # Load the dataset from file\n",
    "    df = file_to_dataframe(filename)\n",
    "    data = df[columns]\n",
    "    \n",
    "    # Replace infinite values with specified value\n",
    "    data.replace([np.inf, -np.inf], change_inf, inplace=True)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    datas = scaler.fit_transform(data)\n",
    "\n",
    "    data = pd.DataFrame(datas, columns=data.columns)\n",
    "\n",
    "    # Select a random 10% sample of the data\n",
    "    indices = np.random.choice(range(len(data)), size=int(len(data) * 0.1), replace=False)\n",
    "    data = data.iloc[indices]\n",
    "    data_id_abo = df[['ID_ABONNE']].iloc[indices]\n",
    "\n",
    "    # Replace NaN values with specified value\n",
    "    data.replace(np.nan, change_nan, inplace=True)\n",
    "\n",
    "    silhouette_scores = []\n",
    "\n",
    "    with PdfPages(data_path_results + \"silhouettes_scores.pdf\") as pdf:\n",
    "        for n_clusters in range_n_clusters:\n",
    "\n",
    "            # Initialize the clusterer with n_clusters value and a random generator\n",
    "            # seed of 10 for reproducibility.\n",
    "            clusterer = KMeans(n_clusters, random_state=10)\n",
    "            clusterer.fit(data)\n",
    "\n",
    "            centers = clusterer.cluster_centers_\n",
    "            centers_cluster = np.round(scaler.inverse_transform(centers), decimals=2)\n",
    "\n",
    "            # Create DataFrame with cluster information\n",
    "            df_cluster = data_frame_cluster(data, columns, scaler.inverse_transform(centers),\n",
    "                                             clusterer.labels_, data_id_abo)\n",
    "            \n",
    "            # Save CSV \n",
    "            save_to_csv_file(df_cluster, data_path_results + \"cluster\" + str(n_clusters) + \".csv\", str(n_clusters))\n",
    "\n",
    "            cluster_labels = clusterer.fit_predict(data)\n",
    "\n",
    "            silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "            print(\n",
    "                \"For n_clusters =\", n_clusters,\n",
    "                \"The average silhouette_score is :\", silhouette_avg,\n",
    "            )\n",
    "\n",
    "            fig, ax1 = plt.subplots(1, 1)\n",
    "            fig.set_size_inches(9, 7)\n",
    "\n",
    "            ax1.set_xlim([-0.1, 1])\n",
    "            ax1.set_ylim([0, len(data) + (n_clusters + 1) * 10])\n",
    "\n",
    "            y_lower = 10\n",
    "            sample_silhouette_values = silhouette_samples(data, cluster_labels)\n",
    "\n",
    "            for i in range(n_clusters):\n",
    "\n",
    "                ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "                ith_cluster_silhouette_values.sort()\n",
    "                size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "                y_upper = y_lower + size_cluster_i\n",
    "                color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "                ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color,\n",
    "                                  edgecolor=color, alpha=0.7)\n",
    "                ax1.text(-0.8, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "                y_lower = y_upper + 10\n",
    "\n",
    "            ax1.set_title(\"The silhouette plot for the various clusters.\" + str(n_clusters))\n",
    "            ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "            ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "            ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "            ax1.set_yticks([])\n",
    "            ax1.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "    return silhouette_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
