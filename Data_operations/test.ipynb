{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_antoine = \"/Users/antoine/Documents/ENSAE2A/Codeperso/everything/Statappperso/Ressources/\"\n",
    "path_results_antoine = \"/Users/antoine/Documents/ENSAE2A/Codeperso/everything/Statappperso/Ressources/Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from Tool_Functions.cleaning_data import *\n",
    "from Tool_Functions.join_data import *\n",
    "from Tool_Functions.test_comportment_reabo import *\n",
    "from Tool_Functions.visual import *\n",
    "from Tool_Functions.comportment_reabo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans  # Import KMeans clustering algorithm\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score  # Import silhouette metrics\n",
    "\n",
    "import matplotlib.cm as cm  # Import colormap for visualizations\n",
    "import matplotlib.pyplot as plt  # Import plotting library\n",
    "import numpy as np  \n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages  # Import PdfPages for saving plots to PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_to_dataframe(path_antoine + 'fusion_table_score_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID_ABONNE', 'Autres_n_REABOS', 'ODD 15 jours EV+_n_REABOS',\n",
       "       'ODD 15 jours TC_n_REABOS', 'ODD 21 jours TC_n_REABOS',\n",
       "       'ODD 30 jours EV+_n_REABOS', 'ODD 30 jours TC_n_REABOS',\n",
       "       'ODD 7 jours autre que SG_n_REABOS', 'PAS_ODD_n_REABOS',\n",
       "       'Semaine genéreuse_n_REABOS', 'PAS_ODD_MEAN_TIME', 'MOY_DELAI',\n",
       "       'NB_APPARITIONS', 'Autres_MEAN_TIME_DIFF',\n",
       "       'ODD 15 jours EV+_MEAN_TIME_DIFF', 'ODD 15 jours TC_MEAN_TIME_DIFF',\n",
       "       'ODD 21 jours TC_MEAN_TIME_DIFF', 'ODD 30 jours EV+_MEAN_TIME_DIFF',\n",
       "       'ODD 30 jours TC_MEAN_TIME_DIFF',\n",
       "       'ODD 7 jours autre que SG_MEAN_TIME_DIFF', 'PAS_ODD_MEAN_TIME_DIFF',\n",
       "       'Semaine genéreuse_MEAN_TIME_DIFF', 'Autres_n_REABOS_POURCENTAGE',\n",
       "       'ODD 15 jours EV+_n_REABOS_POURCENTAGE',\n",
       "       'ODD 15 jours TC_n_REABOS_POURCENTAGE',\n",
       "       'ODD 21 jours TC_n_REABOS_POURCENTAGE',\n",
       "       'ODD 30 jours EV+_n_REABOS_POURCENTAGE',\n",
       "       'ODD 30 jours TC_n_REABOS_POURCENTAGE',\n",
       "       'ODD 7 jours autre que SG_n_REABOS_POURCENTAGE',\n",
       "       'Semaine genéreuse_n_REABOS_POURCENTAGE', '1', '2', '3', '4', '5', '6',\n",
       "       '7', '8', '9', '10', '11', '12', 'NOMBRE_ABONNEMENTS',\n",
       "       'SCORE_FIDELITE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scales the data set\n",
    "\n",
    "def cluster_data_set_pourc(filename, columns, change_inf=np.nan, change_nan=15):\n",
    "    \"\"\"\n",
    "    Load a dataset, perform clustering, and return clustered data.\n",
    "\n",
    "    Args:\n",
    "    - filename: Name of the file containing the data.\n",
    "    - columns: List of column names to include in clustering.\n",
    "    - change_inf: Value to use for replacing infinity.\n",
    "    - change_nan: Value to use for replacing NaN.\n",
    "\n",
    "    Returns:\n",
    "    - data: DataFrame containing clustered data.\n",
    "    \"\"\"\n",
    "    # Load the dataset from file\n",
    "    df = file_to_dataframe(filename)\n",
    "    \n",
    "    # Select the specified columns\n",
    "    data = df[columns]\n",
    "    \n",
    "    # Replace infinite values with specified value\n",
    "    data.replace([np.inf, -np.inf], change_inf, inplace=True)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    datas = scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(datas, columns=data.columns)\n",
    "\n",
    "    # Select a random 10% sample of the data\n",
    "    indices = np.random.choice(range(len(data)), size=int(len(data) * 0.1), replace=False)\n",
    "    data = data.iloc[indices]\n",
    "\n",
    "    # Replace NaN values with specified value\n",
    "    data.replace(np.nan, change_nan, inplace=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scales the data set\n",
    "\n",
    "def cluster_data_set(filename, columns, change_inf=np.nan, change_nan=15):\n",
    "    \"\"\"\n",
    "    Load a dataset, perform clustering, and return clustered data.\n",
    "\n",
    "    Args:\n",
    "    - filename: Name of the file containing the data.\n",
    "    - columns: List of column names to include in clustering.\n",
    "    - change_inf: Value to use for replacing infinity.\n",
    "    - change_nan: Value to use for replacing NaN.\n",
    "\n",
    "    Returns:\n",
    "    - data: DataFrame containing clustered data.\n",
    "    \"\"\"\n",
    "    # Load the dataset from file\n",
    "    df = file_to_dataframe(filename)\n",
    "    \n",
    "    # Select the specified columns\n",
    "    data = df[columns]\n",
    "    \n",
    "    # Replace infinite values with specified value\n",
    "    data.replace([np.inf, -np.inf], change_inf, inplace=True)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    datas = scaler.fit_transform(datas)\n",
    "    data = pd.DataFrame(datas, columns=data.columns)\n",
    "\n",
    "    # Select a random 10% sample of the data\n",
    "\n",
    "    # Replace NaN values with specified value\n",
    "    data.replace(np.nan, change_nan, inplace=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Après avoir utilisé KMeans et avoir obtenu clusters = ...\n",
    "\n",
    "def data_frame_cluster(data, columns, centers_inv, clusters, data_id_abo):\n",
    "    \"\"\"\n",
    "    Create a DataFrame containing clusters, cluster centers, and subscriber IDs.\n",
    "\n",
    "    Args:\n",
    "    - data: DataFrame containing the data.\n",
    "    - columns: List of column names.\n",
    "    - centers_inv: Inverse cluster centers (denormalized).\n",
    "    - clusters: Cluster number assigned to each sample.\n",
    "    - data_id_abo: DataFrame containing subscriber IDs.\n",
    "\n",
    "    Returns:\n",
    "    - df_clusters: DataFrame containing clusters, centers, and percentage IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    datas = data.copy()\n",
    "    # Add cluster labels and subscriber IDs to the data\n",
    "    datas['KMEANS'] = clusters\n",
    "    datas['ID_ABONNE'] = data_id_abo['ID_ABONNE']\n",
    "\n",
    "    # Generate DataFrame with cluster information\n",
    "    df_clusters = percent_abo_conditions(datas, 'KMEANS', 'ID_ABONNE')\n",
    "    df_clusters = df_clusters.sort_values(by='KMEANS')\n",
    "\n",
    "    # Round and assign cluster centers to the DataFrame\n",
    "    centers = np.round(centers_inv, decimals=2)\n",
    "    for j in range(len(columns)):\n",
    "        df_clusters[columns[j]] = [centers[i][j] for i in range(len(centers))]\n",
    "\n",
    "    return df_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_silhouette_datas_all(filename, columns, range_n_clusters, data_path_results, change_inf=np.nan, change_nan=15):\n",
    "    \"\"\"\n",
    "    Visualize silhouette scores for different numbers of clusters.\n",
    "\n",
    "    Args:\n",
    "    - filename: Name of the file containing the data.\n",
    "    - columns: List of column names to include in clustering.\n",
    "    - range_n_clusters: List of numbers of clusters to test.\n",
    "    - data_path_results: Path to save the results.\n",
    "    - change_inf: Value to use for replacing infinity.\n",
    "    - change_nan: Value to use for replacing NaN.\n",
    "\n",
    "    Returns:\n",
    "    - silhouette_scores: List of silhouette scores.\n",
    "    \"\"\"\n",
    "    # Load the dataset from file\n",
    "    df = file_to_dataframe(filename)\n",
    "    data = df[columns]\n",
    "    \n",
    "    # Replace infinite values with specified value\n",
    "    data.replace([np.inf, -np.inf], change_inf, inplace=True)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    datas = scaler.fit_transform(data)\n",
    "\n",
    "    data = pd.DataFrame(datas, columns=data.columns)\n",
    "\n",
    "    # Select a random 10% sample of the data\n",
    "    indices = np.random.choice(range(len(data)), size=int(len(data) * 0.1), replace=False)\n",
    "    data = data.iloc[indices]\n",
    "    data_id_abo = df[['ID_ABONNE']].iloc[indices]   \n",
    "\n",
    "    # Replace NaN values with specified value\n",
    "    data.replace(np.nan, change_nan, inplace=True)\n",
    "\n",
    "    silhouette_scores = []\n",
    "\n",
    "    with PdfPages(data_path_results + \"silhouettes_scores_reu.pdf\") as pdf:\n",
    "        for n_clusters in range_n_clusters:\n",
    "\n",
    "            # Initialize the clusterer with n_clusters value and a random generator\n",
    "            # seed of 10 for reproducibility.\n",
    "            clusterer = KMeans(n_clusters, random_state=10)\n",
    "            clusterer.fit(data)\n",
    "\n",
    "            centers = clusterer.cluster_centers_\n",
    "            centers_cluster = np.round(scaler.inverse_transform(centers), decimals=2)\n",
    "\n",
    "            # Create DataFrame with cluster information\n",
    "            df_cluster = data_frame_cluster(data, columns, scaler.inverse_transform(centers),\n",
    "                                             clusterer.labels_, data_id_abo)\n",
    "            \n",
    "            # Write cluster DataFrame to Excel\n",
    "            save_to_csv_file(df_cluster, data_path_results + \"cluster_reu\" + str(n_clusters) + \".csv\")\n",
    "\n",
    "            cluster_labels = clusterer.fit_predict(data)\n",
    "\n",
    "            silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "            print(\n",
    "                \"For n_clusters =\", n_clusters,\n",
    "                \"The average silhouette_score is :\", silhouette_avg,\n",
    "            )\n",
    "\n",
    "            fig, ax1 = plt.subplots(1, 1)\n",
    "            fig.set_size_inches(9, 7)\n",
    "\n",
    "            ax1.set_xlim([-0.1, 1])\n",
    "            ax1.set_ylim([0, len(data) + (n_clusters + 1) * 10])\n",
    "\n",
    "            y_lower = 10\n",
    "            sample_silhouette_values = silhouette_samples(data, cluster_labels)\n",
    "\n",
    "            for i in range(n_clusters):\n",
    "\n",
    "                ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "                ith_cluster_silhouette_values.sort()\n",
    "                size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "                y_upper = y_lower + size_cluster_i\n",
    "                color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "                ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color,\n",
    "                                  edgecolor=color, alpha=0.7)\n",
    "                ax1.text(-0.8, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "                y_lower = y_upper + 10\n",
    "\n",
    "            ax1.set_title(\"The silhouette plot for the various clusters.\" + str(n_clusters))\n",
    "            ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "            ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "            ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "            ax1.set_yticks([])\n",
    "            ax1.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "    return silhouette_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_silhouette_scores(silhouette_scores, abscisses):\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(abscisses, silhouette_scores, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('KMeans Silhouette Score')\n",
    "    plt.title('Silhouette Score for Different Numbers of Clusters')\n",
    "    plt.show()\n",
    "\n",
    "    plt.xticks(abscisses)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_silhouette_scores(visualize_silhouette_datas_all(path_antoine + \"fusion_table_score_v1.csv\",\n",
    "                                                       ['ODD 15 jours TC_n_REABOS','Semaine genéreuse_n_REABOS', 'SCORE_FIDELITE'],\n",
    "                                                       [2, 3, 4, 5, 6, 7],\n",
    "                                                       path_antoine + \"Clustering_results/\"), [2, 3, 4, 5, 6, 7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
